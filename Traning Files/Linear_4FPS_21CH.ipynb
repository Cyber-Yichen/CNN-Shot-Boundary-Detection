{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bcd4a7a-e054-46e8-9b83-cfb2de1a4252",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e00a1b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ===== Path config (aligned with Untitled2) =====\n",
    "PROJECT_ROOT = \"movie\"\n",
    "DATA_VERSION = \"v13\"\n",
    "REPORTS_BASE_DIR = f\"{PROJECT_ROOT}/reports\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d566f06-a3fe-42b7-87a7-7d15aa1ce1f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "import openpyxl\n",
    "\n",
    "# Paths and parameters\n",
    "video_dir = f\"{PROJECT_ROOT}/{DATA_VERSION}/dataset/cut\"              # Directory containing V001.mp4, V002.mp4, ...\n",
    "excel_file = f\"{PROJECT_ROOT}/{DATA_VERSION}/dataset/cut.xlsx\"        # Excel file with annotations\n",
    "frame_size = (224, 224)                                        # resize frames for CNN input\n",
    "\n",
    "# -------- 读取 Excel（用 openpyxl，替代 pandas） --------\n",
    "wb = openpyxl.load_workbook(excel_file, data_only=True)\n",
    "ws = wb.active  # 默认第一个工作表\n",
    "\n",
    "rows = list(ws.iter_rows(values_only=True))\n",
    "if len(rows) == 0:\n",
    "    raise RuntimeError(\"Excel 里没有任何行！\")\n",
    "\n",
    "# 第 1 行：全局帧率（例如 24）\n",
    "fps_row = rows[0]\n",
    "\n",
    "def get_fps_from_row(r, default=24.0):\n",
    "    for cell in r:\n",
    "        if cell is None:\n",
    "            continue\n",
    "        if isinstance(cell, (int, float)):\n",
    "            return float(cell)\n",
    "        if isinstance(cell, str):\n",
    "            s = cell.strip()\n",
    "            if s == \"\" or s.lower() == \"none\":\n",
    "                continue\n",
    "            try:\n",
    "                return float(s)\n",
    "            except Exception:\n",
    "                continue\n",
    "    return float(default)\n",
    "\n",
    "fps = get_fps_from_row(fps_row, default=24.0)\n",
    "print(f\"Using FPS from Excel first row: {fps}\")\n",
    "\n",
    "# 后面的每一行对应一个视频：这一行的每一列都是一个 cut 的 timecode\n",
    "data_rows = rows[1:]\n",
    "\n",
    "# rows[i] 对应 V00{i+1}\n",
    "video_files = sorted(glob.glob(f\"{video_dir}/V*.mp4\"))\n",
    "assert len(video_files) == len(data_rows), \\\n",
    "    f\"Mismatch between number of videos ({len(video_files)}) and Excel data rows ({len(data_rows)})\"\n",
    "\n",
    "\n",
    "def timecode_to_frame(tc, fps):\n",
    "    \"\"\"\n",
    "    把 Excel 里的单个标注值统一本成帧号：\n",
    "    - None / 'none' / '' → None（表示没有 cut）\n",
    "    - 纯数字字符串 / 数字 → 直接当作帧号\n",
    "    - 'ss:ff' → 秒 + 帧（例如 01:12 在 24fps 下就是 36）\n",
    "    - 'hh:mm:ss' → 标准时码（按秒算：((h*60+m)*60+s)*fps）\n",
    "    \"\"\"\n",
    "    if tc is None:\n",
    "        return None\n",
    "\n",
    "    if isinstance(tc, (int, float)):\n",
    "        return int(tc)\n",
    "\n",
    "    if isinstance(tc, str):\n",
    "        s = tc.strip()\n",
    "        if s == \"\" or s.lower() == \"none\":\n",
    "            return None\n",
    "\n",
    "        if s.isdigit():\n",
    "            return int(s)\n",
    "\n",
    "        if \":\" in s:\n",
    "            parts = s.split(\":\")\n",
    "            try:\n",
    "                if len(parts) == 2:\n",
    "                    sec = int(parts[0])\n",
    "                    frm = int(parts[1])\n",
    "                    return int(sec * fps + frm)\n",
    "                elif len(parts) == 3:\n",
    "                    h = int(parts[0])\n",
    "                    m = int(parts[1])\n",
    "                    sec = int(parts[2])\n",
    "                    total_sec = h * 3600 + m * 60 + sec\n",
    "                    return int(total_sec * fps)\n",
    "                else:\n",
    "                    return None\n",
    "            except Exception:\n",
    "                return None\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# -------- 准备数据容器（这些变量后面 Cell2 会用） --------\n",
    "video_frames = []                 # list[list[np.ndarray]]: 每个视频的全部帧（已resize）\n",
    "boundary_pairs = []               # list[tuple]: (vid_idx, i, label)\n",
    "boundary_pairs_by_video = []      # list[list[tuple]]: 每个视频自己的pairs（便于后面更聪明采样）\n",
    "cut_indices_list = []             # list[list[int]]: 每个视频的 cut_indices（i位置，表示 i 和 i+1 之间有切）\n",
    "total_frames_list = []            # list[int]: 每个视频总帧数\n",
    "valid_video_mask = []             # list[bool]: 视频是否有效（>=2帧）\n",
    "video_paths = list(video_files)   # 备份一下路径，后面打印/定位很方便\n",
    "\n",
    "# -------- 遍历视频 + 对应 Excel 行 --------\n",
    "for vid_idx, (video_path, row) in enumerate(zip(video_files, data_rows)):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    success, frame = cap.read()\n",
    "    while success:\n",
    "        frame_resized = cv2.resize(frame, frame_size)\n",
    "        frames.append(frame_resized)\n",
    "        success, frame = cap.read()\n",
    "    cap.release()\n",
    "\n",
    "    video_frames.append(frames)\n",
    "    total_frames = len(frames)\n",
    "    total_frames_list.append(total_frames)\n",
    "\n",
    "    if total_frames <= 1:\n",
    "        valid_video_mask.append(False)\n",
    "        boundary_pairs_by_video.append([])\n",
    "        cut_indices_list.append([])\n",
    "        print(f\"Warning: video {video_path} has {total_frames} frame(s), skip boundary generation.\")\n",
    "        continue\n",
    "\n",
    "    valid_video_mask.append(True)\n",
    "\n",
    "    raw_values = list(row) if row is not None else []\n",
    "\n",
    "    cut_indices = []\n",
    "    for v in raw_values:\n",
    "        frame_idx = timecode_to_frame(v, fps)\n",
    "        if frame_idx is None:\n",
    "            continue\n",
    "\n",
    "        # Excel 标的是切后镜头起始帧 (B-start)\n",
    "        # SBD 的 cut 应标在 (B-1, B) 之间 → 对应 pair i = B-1\n",
    "        if frame_idx > 0:\n",
    "            frame_idx = frame_idx - 1\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        frame_idx = int(frame_idx)\n",
    "        frame_idx = max(0, min(frame_idx, total_frames - 2))\n",
    "        cut_indices.append(frame_idx)\n",
    "\n",
    "    cut_indices = sorted(set(cut_indices))\n",
    "    cut_indices_list.append(cut_indices)\n",
    "\n",
    "    print(f\"Video {vid_idx} ({video_path}): total_frames={total_frames}, cuts-1@frames={cut_indices}\")\n",
    "\n",
    "    cut_set = set(cut_indices)\n",
    "\n",
    "    per_video_pairs = []\n",
    "    for i in range(total_frames - 1):\n",
    "        label = 1 if i in cut_set else 0\n",
    "        tup = (vid_idx, i, label)\n",
    "        boundary_pairs.append(tup)\n",
    "        per_video_pairs.append(tup)\n",
    "\n",
    "    boundary_pairs_by_video.append(per_video_pairs)\n",
    "\n",
    "# -------- 关键全局变量（Cell2 会直接用到） --------\n",
    "num_videos = len(video_files)\n",
    "\n",
    "# -------- 数据统计 --------\n",
    "num_pairs = len(boundary_pairs)\n",
    "num_cuts = sum(1 for _, _, lbl in boundary_pairs if lbl == 1)\n",
    "num_noncuts = num_pairs - num_cuts\n",
    "\n",
    "total_extracted_frames = sum(len(frames) for frames in video_frames)\n",
    "num_valid_videos = sum(1 for v in valid_video_mask if v)\n",
    "\n",
    "print(f\"\\nProcessed {num_videos} videos (valid {num_valid_videos}/{num_videos}), extracted {total_extracted_frames} frames.\")\n",
    "print(f\"Generated {num_pairs} frame pairs: {num_cuts} cuts (positive) and {num_noncuts} non-cuts (negative).\")\n",
    "\n",
    "# -------- 可选：把全局正负比例也存一下，后续算pos_weight更方便 --------\n",
    "pos_count = num_cuts\n",
    "neg_count = num_noncuts\n",
    "pos_ratio = (pos_count / num_pairs) if num_pairs > 0 else 0.0\n",
    "neg_ratio = (neg_count / num_pairs) if num_pairs > 0 else 0.0\n",
    "print(f\"Pos ratio: {pos_ratio:.6f} | Neg ratio: {neg_ratio:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdaa218-73cd-446b-8e0d-13d47e3c2c80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============= Cell2: Dataset + 按视频划分 + 正例全量/负例动态抽样（每epoch不一样） =============\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class ShotBoundaryDataset(Dataset):\n",
    "    def __init__(self, pairs, video_frames):\n",
    "        self.pairs = pairs\n",
    "        self.video_frames = video_frames\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        vid_idx, frame_idx, label = self.pairs[idx]\n",
    "        frames = self.video_frames[vid_idx]\n",
    "        T = len(frames)\n",
    "\n",
    "        # 4-frame context (clamp to avoid out-of-range)\n",
    "        i_prev = max(0, frame_idx - 1)\n",
    "        i_A    = max(0, min(frame_idx,     T - 1))\n",
    "        i_B    = max(0, min(frame_idx + 1, T - 1))\n",
    "        i_next = max(0, min(frame_idx + 2, T - 1))\n",
    "\n",
    "        framePrev = frames[i_prev]\n",
    "        frameA    = frames[i_A]\n",
    "        frameB    = frames[i_B]\n",
    "        frameNext = frames[i_next]\n",
    "\n",
    "        diff1 = cv2.absdiff(framePrev, frameA)\n",
    "        diff2 = cv2.absdiff(frameA, frameB)\n",
    "        diff3 = cv2.absdiff(frameB, frameNext)\n",
    "\n",
    "        # 21ch = prev(3)+A(3)+B(3)+next(3)+diff(prev,A)(3)+diff(A,B)(3)+diff(B,next)(3)\n",
    "        img_21ch = np.concatenate([framePrev, frameA, frameB, frameNext, diff1, diff2, diff3], axis=2).astype(\"float32\") / 255.0\n",
    "        img_chw = np.transpose(img_21ch, (2, 0, 1))\n",
    "\n",
    "        img_tensor = torch.tensor(img_chw, dtype=torch.float32)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "        return img_tensor, label_tensor\n",
    "\n",
    "\n",
    "# ============= 你只需要改这里的采样参数 =============\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "USE_DYNAMIC_NEG_SAMPLING = False       # True=每个epoch动态抽负例；False=全量训练（等同你原来的）\n",
    "NEG_SAMPLING_MODE = \"ratio\"           # \"ratio\" or \"per_pos\"\n",
    "NEG_SAMPLE_RATIO = 0.10               # mode=\"ratio\": 每个epoch抽取负例池的比例（0.1=十分之一）\n",
    "NEG_PER_POS = 5                       # mode=\"per_pos\": 每个epoch负例数 = 正例数 * NEG_PER_POS\n",
    "\n",
    "SEED_SPLIT = 42                       # 训练/测试划分可复现\n",
    "SEED_EPOCH_BASE = 20260118            # 每个epoch采样可复现（不同epoch会不一样）\n",
    "\n",
    "\n",
    "# ============= 划分训练测试（按视频） =============\n",
    "all_video_indices = list(range(num_videos))\n",
    "random.seed(SEED_SPLIT)\n",
    "random.shuffle(all_video_indices)\n",
    "\n",
    "split_idx = int(len(all_video_indices) * 0.95)\n",
    "train_vids = set(all_video_indices[:split_idx])\n",
    "test_vids  = set(all_video_indices[split_idx:])\n",
    "\n",
    "train_pairs_all = [p for p in boundary_pairs if p[0] in train_vids]\n",
    "test_pairs      = [p for p in boundary_pairs if p[0] in test_vids]\n",
    "\n",
    "# 训练池拆正/负\n",
    "train_pos_pairs = [p for p in train_pairs_all if p[2] == 1]\n",
    "train_neg_pairs = [p for p in train_pairs_all if p[2] == 0]\n",
    "\n",
    "# 测试集：永远全量，不做抽样\n",
    "test_dataset = ShotBoundaryDataset(test_pairs, video_frames)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"[Split] train_vids={len(train_vids)} test_vids={len(test_vids)}\")\n",
    "print(f\"[Pool] train_pos={len(train_pos_pairs)} train_neg={len(train_neg_pairs)} test_total={len(test_pairs)}\")\n",
    "\n",
    "\n",
    "def make_train_loader_for_epoch(epoch: int, batch_size: int = BATCH_SIZE, shuffle: bool = True):\n",
    "    \"\"\"\n",
    "    每个epoch动态构造训练集：\n",
    "    - 正例：全量保留（每轮必学）\n",
    "    - 负例：每轮重新随机抽样（每轮都不一样）\n",
    "    \"\"\"\n",
    "    if (not USE_DYNAMIC_NEG_SAMPLING) or (len(train_neg_pairs) == 0):\n",
    "        epoch_pairs = list(train_pairs_all)\n",
    "        rng = None\n",
    "    else:\n",
    "        rng = random.Random(SEED_EPOCH_BASE + int(epoch))\n",
    "\n",
    "        if NEG_SAMPLING_MODE == \"ratio\":\n",
    "            k = int(len(train_neg_pairs) * float(NEG_SAMPLE_RATIO))\n",
    "        elif NEG_SAMPLING_MODE == \"per_pos\":\n",
    "            k = int(len(train_pos_pairs) * int(NEG_PER_POS))\n",
    "        else:\n",
    "            raise ValueError(\"NEG_SAMPLING_MODE must be 'ratio' or 'per_pos'\")\n",
    "\n",
    "        k = max(1, min(k, len(train_neg_pairs)))\n",
    "        neg_sample = rng.sample(train_neg_pairs, k)\n",
    "\n",
    "        epoch_pairs = list(train_pos_pairs) + neg_sample\n",
    "        rng.shuffle(epoch_pairs)\n",
    "\n",
    "    train_dataset_epoch = ShotBoundaryDataset(epoch_pairs, video_frames)\n",
    "    train_loader_epoch  = DataLoader(train_dataset_epoch, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    pos_n = len(train_pos_pairs)\n",
    "    neg_n = len(train_dataset_epoch) - pos_n\n",
    "    raw_pos_weight_epoch = (neg_n / max(1, pos_n))\n",
    "\n",
    "    # 给你一个非常直观的确认：每轮neg数量、raw_pos_weight都会按你的抽样变化\n",
    "    print(f\"[Epoch {epoch}] train_pairs={len(train_dataset_epoch)} (pos {pos_n}, neg {neg_n}) raw_pos_weight_ep={raw_pos_weight_epoch:.4f}\")\n",
    "    return train_loader_epoch, train_dataset_epoch, raw_pos_weight_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66011fa-3e95-48cd-a33a-fc254c4d5a4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============= Cell3-L: Linear Baseline 训练（1层网络：Pool + Linear）+ 每Epoch输出多指标 + 写Excel =============\n",
    "\n",
    "import os\n",
    "import platform\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "from datetime import datetime\n",
    "\n",
    "try:\n",
    "    from zoneinfo import ZoneInfo  # py>=3.9\n",
    "    _TZ = ZoneInfo(\"Asia/BeiJing\")\n",
    "except Exception:\n",
    "    _TZ = None\n",
    "\n",
    "# ===== 简单版 classification_report（沿用你的）=====\n",
    "def simple_classification_report(y_true, y_pred, target_names):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    n_classes = len(target_names)\n",
    "    lines = []\n",
    "    acc = (y_true == y_pred).sum() / len(y_true) if len(y_true) > 0 else 0.0\n",
    "    lines.append(\"precision    recall  f1-score   support\")\n",
    "    for i in range(n_classes):\n",
    "        name = target_names[i]\n",
    "        true_i = (y_true == i)\n",
    "        pred_i = (y_pred == i)\n",
    "        tp = np.logical_and(true_i, pred_i).sum()\n",
    "        fp = np.logical_and(~true_i, pred_i).sum()\n",
    "        fn = np.logical_and(true_i, ~pred_i).sum()\n",
    "        support = true_i.sum()\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall    = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1        = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        lines.append(f\"{name:10s}  {precision:0.4f}   {recall:0.4f}   {f1:0.4f}   {support:5d}\")\n",
    "    lines.append(f\"\\naccuracy                        {acc:0.4f}   {len(y_true):5d}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# ===== 二分类指标（沿用你的）=====\n",
    "def _safe_div(a, b):\n",
    "    return float(a) / float(b) if b else 0.0\n",
    "\n",
    "def binary_metrics_from_probs(y_true, prob_pos, threshold=0.95):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    prob_pos = np.asarray(prob_pos, dtype=np.float64)\n",
    "    y_pred = (prob_pos >= threshold).astype(np.int64)\n",
    "\n",
    "    tp = int(np.sum((y_true == 1) & (y_pred == 1)))\n",
    "    fp = int(np.sum((y_true == 0) & (y_pred == 1)))\n",
    "    tn = int(np.sum((y_true == 0) & (y_pred == 0)))\n",
    "    fn = int(np.sum((y_true == 1) & (y_pred == 0)))\n",
    "\n",
    "    precision = _safe_div(tp, tp + fp)\n",
    "    recall    = _safe_div(tp, tp + fn)\n",
    "    f1        = _safe_div(2 * precision * recall, precision + recall)\n",
    "    acc       = _safe_div(tp + tn, tp + tn + fp + fn)\n",
    "\n",
    "    pos_pred_rate = _safe_div(tp + fp, len(y_true))\n",
    "\n",
    "    avg_prob_pos = float(np.mean(prob_pos[y_true == 1])) if np.any(y_true == 1) else 0.0\n",
    "    avg_prob_neg = float(np.mean(prob_pos[y_true == 0])) if np.any(y_true == 0) else 0.0\n",
    "\n",
    "    return {\n",
    "        \"threshold\": float(threshold),\n",
    "        \"tp\": tp, \"fp\": fp, \"tn\": tn, \"fn\": fn,\n",
    "        \"precision\": precision, \"recall\": recall, \"f1\": f1, \"acc\": acc,\n",
    "        \"pos_pred_rate\": pos_pred_rate,\n",
    "        \"avg_prob_pos\": avg_prob_pos,\n",
    "        \"avg_prob_neg\": avg_prob_neg,\n",
    "    }\n",
    "\n",
    "def average_precision_score(y_true, prob_pos):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    prob_pos = np.asarray(prob_pos, dtype=np.float64)\n",
    "    pos_count = int(np.sum(y_true == 1))\n",
    "    if pos_count == 0:\n",
    "        return 0.0\n",
    "    order = np.argsort(-prob_pos)\n",
    "    y_sorted = y_true[order]\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    precisions_at_hits = []\n",
    "    for i in range(len(y_sorted)):\n",
    "        if y_sorted[i] == 1:\n",
    "            tp += 1\n",
    "            precisions_at_hits.append(tp / (tp + fp))\n",
    "        else:\n",
    "            fp += 1\n",
    "    return float(np.sum(precisions_at_hits) / pos_count)\n",
    "\n",
    "def roc_auc_score_rank(y_true, prob_pos):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    prob_pos = np.asarray(prob_pos, dtype=np.float64)\n",
    "    n_pos = int(np.sum(y_true == 1))\n",
    "    n_neg = int(np.sum(y_true == 0))\n",
    "    if n_pos == 0 or n_neg == 0:\n",
    "        return 0.0\n",
    "\n",
    "    order = np.argsort(prob_pos)\n",
    "    ranks = np.empty_like(order, dtype=np.float64)\n",
    "    ranks[order] = np.arange(1, len(prob_pos) + 1, dtype=np.float64)\n",
    "\n",
    "    sorted_scores = prob_pos[order]\n",
    "    i = 0\n",
    "    while i < len(sorted_scores):\n",
    "        j = i\n",
    "        while j + 1 < len(sorted_scores) and sorted_scores[j + 1] == sorted_scores[i]:\n",
    "            j += 1\n",
    "        if j > i:\n",
    "            avg_rank = float(np.mean(ranks[order[i:j+1]]))\n",
    "            ranks[order[i:j+1]] = avg_rank\n",
    "        i = j + 1\n",
    "\n",
    "    sum_ranks_pos = float(np.sum(ranks[y_true == 1]))\n",
    "    auc = (sum_ranks_pos - n_pos * (n_pos + 1) / 2.0) / (n_pos * n_neg)\n",
    "    return float(auc)\n",
    "\n",
    "def find_best_threshold_f1(y_true, prob_pos, num_thresholds=101):\n",
    "    best_t = 0.5\n",
    "    best_f1 = -1.0\n",
    "    for k in range(num_thresholds):\n",
    "        t = k / (num_thresholds - 1)\n",
    "        m = binary_metrics_from_probs(y_true, prob_pos, threshold=t)\n",
    "        if m[\"f1\"] > best_f1:\n",
    "            best_f1 = m[\"f1\"]\n",
    "            best_t = t\n",
    "    return float(best_t), float(best_f1)\n",
    "\n",
    "# ===== 设备 =====\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ===== Linear Baseline：无参数池化 + 1层Linear（唯一可学习层）=====\n",
    "class BoundaryLinearBaseline(nn.Module):\n",
    "    def __init__(self, in_ch=21, pool_hw=16):\n",
    "        super().__init__()\n",
    "        self.pool = nn.AdaptiveAvgPool2d((pool_hw, pool_hw))  # 无参数\n",
    "        self.fc = nn.Linear(in_ch * pool_hw * pool_hw, 2)     # 仅这一层有参数\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "baseline_model = BoundaryLinearBaseline(in_ch=21, pool_hw=16).to(device)\n",
    "\n",
    "# ===== 类别不平衡权重：保持你原逻辑（用全量统计的 num_cuts/num_noncuts）=====\n",
    "#if num_cuts > 0:\n",
    "    #raw_pos_weight = num_noncuts / num_cuts\n",
    "    # 你 CNN 里手动写 pos_weight=90，这里也沿用（对比更“同条件”）\n",
    "    #pos_weight = 40.0\n",
    "#else:\n",
    "    #raw_pos_weight = None\n",
    "    #pos_weight = 1.0\n",
    "\n",
    "\n",
    "# ===== 和你一致：POS_WEIGHT_MODE 仍保留，但默认 fixed（也就是不随epoch变）=====\n",
    "POS_WEIGHT_MODE = \"fixed\"      # \"fixed\" or \"epoch\"\n",
    "POS_WEIGHT_EPOCH_MAX = 40.0    # epoch模式上限\n",
    "threshold_default = 0.95\n",
    "pos_weight = 40.0\n",
    "raw_pos_weight = num_noncuts / num_cuts\n",
    "print(f\"raw_pos_weight = {num_noncuts / num_cuts if num_cuts > 0 else 'NA'}, used pos_weight = {pos_weight}\")\n",
    "\n",
    "# ===== 优化器：保持简单（你说不用考虑学习好不好，就是对照）=====\n",
    "lr_init = 1e-5\n",
    "optimizer_b = optim.Adam(baseline_model.parameters(), lr=lr_init)\n",
    "\n",
    "def get_system_info():\n",
    "    info = {}\n",
    "    info[\"time_start\"] = datetime.now(_TZ).strftime(\"%Y-%m-%d %H:%M:%S %Z\") if _TZ else datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    info[\"python_version\"] = platform.python_version()\n",
    "    info[\"platform\"] = platform.platform()\n",
    "    info[\"processor\"] = platform.processor()\n",
    "    info[\"torch_version\"] = torch.__version__\n",
    "    info[\"cuda_available\"] = str(torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        try:\n",
    "            info[\"cuda_device_count\"] = str(torch.cuda.device_count())\n",
    "            info[\"cuda_device_name_0\"] = torch.cuda.get_device_name(0)\n",
    "        except Exception:\n",
    "            info[\"cuda_device_count\"] = \"NA\"\n",
    "            info[\"cuda_device_name_0\"] = \"NA\"\n",
    "    else:\n",
    "        info[\"cuda_device_count\"] = \"0\"\n",
    "        info[\"cuda_device_name_0\"] = \"NA\"\n",
    "    info[\"device_used\"] = str(device)\n",
    "    return info\n",
    "\n",
    "run_info = get_system_info()\n",
    "\n",
    "# ===== 训练 =====\n",
    "epochs = 100  # 你要对比的话，可改成跟CNN同样轮数\n",
    "epoch_rows = []\n",
    "global_step = 0\n",
    "t0 = time.time()\n",
    "\n",
    "baseline_model.train()\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # 每个epoch动态构造 train_loader（来自Cell2）\n",
    "    train_loader, train_dataset, raw_pos_weight_epoch = make_train_loader_for_epoch(\n",
    "        epoch, batch_size=BATCH_SIZE, shuffle=True\n",
    "    )\n",
    "\n",
    "    seed_epoch = (int(SEED_EPOCH_BASE) + int(epoch)) if USE_DYNAMIC_NEG_SAMPLING else None\n",
    "    print(f\"[Epoch {epoch}] seed_for_neg_sampling = {seed_epoch}\")\n",
    "\n",
    "    # 每epoch选择 used_pos_weight（与你CNN一致写法）\n",
    "    if POS_WEIGHT_MODE == \"fixed\":\n",
    "        used_pos_weight_epoch = float(pos_weight)\n",
    "    elif POS_WEIGHT_MODE == \"epoch\":\n",
    "        used_pos_weight_epoch = float(min(raw_pos_weight_epoch, POS_WEIGHT_EPOCH_MAX))\n",
    "    else:\n",
    "        used_pos_weight_epoch = float(pos_weight)  # 不可用时兜底\n",
    "\n",
    "    # CrossEntropyLoss(weight=[noncut, cut])（与你CNN一致）\n",
    "    class_weights = torch.tensor([1.0, used_pos_weight_epoch], dtype=torch.float32).to(device)\n",
    "    criterion_b = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    y_true_ep = []\n",
    "    prob_pos_ep = []\n",
    "\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer_b.zero_grad()\n",
    "        outputs = baseline_model(imgs)   # logits [B,2]\n",
    "        loss = criterion_b(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_b.step()\n",
    "\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        global_step += 1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            probs = torch.softmax(outputs, dim=1)[:, 1]\n",
    "            prob_pos_ep.extend(probs.detach().cpu().numpy().tolist())\n",
    "            y_true_ep.extend(labels.detach().cpu().numpy().tolist())\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "    m = binary_metrics_from_probs(y_true_ep, prob_pos_ep, threshold=threshold_default)\n",
    "    ap = average_precision_score(y_true_ep, prob_pos_ep)\n",
    "    auc = roc_auc_score_rank(y_true_ep, prob_pos_ep)\n",
    "    best_t, best_f1 = find_best_threshold_f1(y_true_ep, prob_pos_ep, num_thresholds=101)\n",
    "\n",
    "    lr_now = optimizer_b.param_groups[0].get(\"lr\", None)\n",
    "\n",
    "    print(\n",
    "        f\"[Linear] Epoch {epoch+1}/{epochs} | \"\n",
    "        f\"loss {epoch_loss:.4f} | \"\n",
    "        f\"F1 {m['f1']:.4f} (P {m['precision']:.4f}, R {m['recall']:.4f}) | \"\n",
    "        f\"AP {ap:.4f} | AUC {auc:.4f} | \"\n",
    "        f\"TP {m['tp']} FP {m['fp']} TN {m['tn']} FN {m['fn']} | \"\n",
    "        f\"pos_pred_rate {m['pos_pred_rate']:.4f} | \"\n",
    "        f\"best_t(train) {best_t:.2f} bestF1(train) {best_f1:.4f} | \"\n",
    "        f\"pos_weight {used_pos_weight_epoch:.2f} (raw_ep {raw_pos_weight_epoch:.2f}) | \"\n",
    "        f\"train_pairs {len(train_loader.dataset)} \"\n",
    "        f\"(pos {len(train_pos_pairs)}, neg {len(train_loader.dataset)-len(train_pos_pairs)}) | \"\n",
    "        f\"neg_sampling {USE_DYNAMIC_NEG_SAMPLING} {NEG_SAMPLING_MODE} \"\n",
    "        f\"ratio {NEG_SAMPLE_RATIO} per_pos {NEG_PER_POS}\"\n",
    "    )\n",
    "\n",
    "    # ===== epoch_rows：列名/顺序对齐你CNN Cell3 的写法 =====\n",
    "    epoch_rows.append({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"global_step\": global_step,\n",
    "        \"loss\": float(epoch_loss),\n",
    "        \"threshold\": float(threshold_default),\n",
    "        \"precision\": float(m[\"precision\"]),\n",
    "        \"recall\": float(m[\"recall\"]),\n",
    "        \"f1\": float(m[\"f1\"]),\n",
    "        \"acc\": float(m[\"acc\"]),\n",
    "        \"tp\": int(m[\"tp\"]),\n",
    "        \"fp\": int(m[\"fp\"]),\n",
    "        \"tn\": int(m[\"tn\"]),\n",
    "        \"fn\": int(m[\"fn\"]),\n",
    "        \"pos_pred_rate\": float(m[\"pos_pred_rate\"]),\n",
    "        \"avg_prob_pos\": float(m[\"avg_prob_pos\"]),\n",
    "        \"avg_prob_neg\": float(m[\"avg_prob_neg\"]),\n",
    "        \"pr_auc_ap\": float(ap),\n",
    "        \"roc_auc\": float(auc),\n",
    "        \"best_threshold_train\": float(best_t),\n",
    "        \"best_f1_train\": float(best_f1),\n",
    "        \"lr\": float(lr_now) if lr_now is not None else 0.0,\n",
    "\n",
    "        # ===== 保留原字段含义 =====\n",
    "        \"pos_weight\": float(pos_weight),\n",
    "        \"raw_pos_weight\": float(raw_pos_weight) if raw_pos_weight is not None else 0.0,\n",
    "        \"weight_noncut\": float(class_weights[0].item()) if class_weights is not None else 0.0,\n",
    "        \"weight_cut\": float(class_weights[1].item()) if class_weights is not None else 0.0,\n",
    "\n",
    "        # ===== 追加字段（对齐你CNN的 extra_cols）=====\n",
    "        \"train_pairs_epoch\": int(len(train_loader.dataset)) if train_loader is not None else 0,\n",
    "        \"raw_pos_weight_epoch\": float(raw_pos_weight_epoch) if raw_pos_weight_epoch is not None else 0.0,\n",
    "        \"pos_weight_mode\": str(POS_WEIGHT_MODE) if POS_WEIGHT_MODE is not None else \"unavailable\",\n",
    "        \"pos_weight_used_epoch\": float(used_pos_weight_epoch) if used_pos_weight_epoch is not None else 0.0,\n",
    "        \"neg_sampling_enabled\": bool(USE_DYNAMIC_NEG_SAMPLING),\n",
    "        \"neg_sampling_mode\": str(NEG_SAMPLING_MODE) if NEG_SAMPLING_MODE is not None else \"unavailable\",\n",
    "        \"neg_sample_ratio\": float(NEG_SAMPLE_RATIO) if NEG_SAMPLE_RATIO is not None else 0.0,\n",
    "        \"neg_per_pos\": int(NEG_PER_POS) if NEG_PER_POS is not None else 0,\n",
    "    })\n",
    "\n",
    "print(\"Finished training Linear Baseline model.\")\n",
    "t_train = time.time() - t0\n",
    "\n",
    "# ===== 最终测试（按你CNN Cell3：test_loader 是Cell2里做的视频级切分后的全量）=====\n",
    "baseline_model.eval()\n",
    "y_true_b, prob_pos_b, y_pred_b = [], [], []\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        outputs = baseline_model(imgs)                 # logits [B,2]\n",
    "        probs = torch.softmax(outputs, dim=1)[:, 1]    # P(cut)\n",
    "        pred = (probs >= threshold_default).long()\n",
    "\n",
    "        y_true_b.extend(labels.detach().cpu().numpy().tolist())\n",
    "        prob_pos_b.extend(probs.detach().cpu().numpy().tolist())\n",
    "        y_pred_b.extend(pred.detach().cpu().numpy().tolist())\n",
    "\n",
    "m_test = binary_metrics_from_probs(y_true_b, prob_pos_b, threshold=threshold_default)\n",
    "ap_test = average_precision_score(y_true_b, prob_pos_b)\n",
    "auc_test = roc_auc_score_rank(y_true_b, prob_pos_b)\n",
    "\n",
    "print(\"\\n[Linear] FINAL TEST (threshold=0.95) => \"\n",
    "      f\"F1 {m_test['f1']:.4f} (P {m_test['precision']:.4f}, R {m_test['recall']:.4f}) | \"\n",
    "      f\"AP {ap_test:.4f} | AUC {auc_test:.4f} | \"\n",
    "      f\"TP {m_test['tp']} FP {m_test['fp']} TN {m_test['tn']} FN {m_test['fn']}\")\n",
    "\n",
    "print(\"\\nShot Boundary Detection - Classification Report (TEST):\")\n",
    "print(simple_classification_report(y_true_b, y_pred_b, target_names=[\"Non-cut\", \"Cut\"]))\n",
    "\n",
    "# ===== 写入 Excel 到 movie/reports，结构对齐你CNN Cell3 =====\n",
    "base_dir = \"movie/reports\"\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "ts_folder = datetime.now(_TZ).strftime(\"%m%d%H%M\") if _TZ else datetime.now().strftime(\"%m%d%H%M\")\n",
    "\n",
    "# 保持你原来的命名风格：{time}_e{epochs}_w{pos_weight}\n",
    "# 但为了区分CNN vs Linear，追加一个 suffix，不改变你程序抓 e/w 的方式\n",
    "REPORT_FOLDER_NAME = f\"{ts_folder}_e{epochs}_w{int(pos_weight)}_linear\"\n",
    "REPORT_DIR = os.path.join(base_dir, REPORT_FOLDER_NAME)\n",
    "os.makedirs(REPORT_DIR, exist_ok=True)\n",
    "\n",
    "ts = datetime.now(_TZ).strftime(\"%Y%m%d_%H%M%S\") if _TZ else datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "out_path = os.path.join(REPORT_DIR, f\"boundary_train_metrics_{ts}.xlsx\")\n",
    "\n",
    "wb = openpyxl.Workbook()\n",
    "\n",
    "# Sheet 1: run_info（对齐你CNN的键）\n",
    "ws0 = wb.active\n",
    "ws0.title = \"run_info\"\n",
    "ws0.append([\"key\", \"value\"])\n",
    "for k, v in run_info.items():\n",
    "    ws0.append([k, str(v)])\n",
    "\n",
    "ws0.append([\"\"])\n",
    "ws0.append([\"model_name\", \"BoundaryLinearBaseline\"])\n",
    "ws0.append([\"epochs\", str(epochs)])\n",
    "ws0.append([\"train_seconds\", f\"{t_train:.3f}\"])\n",
    "ws0.append([\"threshold_default\", str(threshold_default)])\n",
    "ws0.append([\"optimizer\", \"Adam\"])\n",
    "ws0.append([\"lr_init\", str(lr_init)])\n",
    "ws0.append([\"loss\", \"CrossEntropyLoss(weighted)\"])\n",
    "ws0.append([\"class_weight_noncut\", str(float(1.0))])\n",
    "ws0.append([\"class_weight_cut\", str(float(pos_weight))])\n",
    "ws0.append([\"raw_pos_weight\", str(raw_pos_weight) if raw_pos_weight is not None else \"0\"])\n",
    "ws0.append([\"pos_weight_used\", str(pos_weight)])\n",
    "\n",
    "ws0.append([\"\"])\n",
    "ws0.append([\"pos_weight_mode\", str(POS_WEIGHT_MODE) if POS_WEIGHT_MODE is not None else \"unavailable\"])\n",
    "ws0.append([\"pos_weight_epoch_max\", str(POS_WEIGHT_EPOCH_MAX) if POS_WEIGHT_EPOCH_MAX is not None else \"0\"])\n",
    "ws0.append([\"neg_sampling_enabled\", str(USE_DYNAMIC_NEG_SAMPLING)])\n",
    "ws0.append([\"neg_sampling_mode\", str(NEG_SAMPLING_MODE) if NEG_SAMPLING_MODE is not None else \"unavailable\"])\n",
    "ws0.append([\"neg_sample_ratio\", str(NEG_SAMPLE_RATIO) if NEG_SAMPLE_RATIO is not None else \"0\"])\n",
    "ws0.append([\"neg_per_pos\", str(NEG_PER_POS) if NEG_PER_POS is not None else \"0\"])\n",
    "\n",
    "ws0.append([\"\"])\n",
    "ws0.append([\"report_folder_name\", REPORT_FOLDER_NAME])\n",
    "ws0.append([\"report_dir\", REPORT_DIR])\n",
    "\n",
    "# Sheet 2: epoch_metrics（列名+顺序对齐你CNN）\n",
    "ws1 = wb.create_sheet(\"epoch_metrics\")\n",
    "cols = [\n",
    "    \"epoch\",\"global_step\",\"loss\",\"lr\",\n",
    "    \"threshold\",\"precision\",\"recall\",\"f1\",\"acc\",\n",
    "    \"tp\",\"fp\",\"tn\",\"fn\",\n",
    "    \"pos_pred_rate\",\"avg_prob_pos\",\"avg_prob_neg\",\n",
    "    \"pr_auc_ap\",\"roc_auc\",\n",
    "    \"best_threshold_train\",\"best_f1_train\",\n",
    "    \"raw_pos_weight\",\"pos_weight\",\"weight_noncut\",\"weight_cut\"\n",
    "]\n",
    "extra_cols = [\n",
    "    \"train_pairs_epoch\",\n",
    "    \"raw_pos_weight_epoch\",\n",
    "    \"pos_weight_mode\",\n",
    "    \"pos_weight_used_epoch\",\n",
    "    \"neg_sampling_enabled\",\n",
    "    \"neg_sampling_mode\",\n",
    "    \"neg_sample_ratio\",\n",
    "    \"neg_per_pos\"\n",
    "]\n",
    "all_cols = cols + extra_cols\n",
    "\n",
    "ws1.append(all_cols)\n",
    "for r in epoch_rows:\n",
    "    row_out = []\n",
    "    for c in all_cols:\n",
    "        v = r.get(c, None)\n",
    "        # 不可用字段处理：字符串 -> unavailable；数值 -> 0\n",
    "        if v is None:\n",
    "            if c in [\"pos_weight_mode\", \"neg_sampling_mode\"]:\n",
    "                v = \"unavailable\"\n",
    "            else:\n",
    "                v = 0\n",
    "        row_out.append(v)\n",
    "    ws1.append(row_out)\n",
    "\n",
    "# Sheet 3: final_test（对齐你CNN）\n",
    "ws2 = wb.create_sheet(\"final_test\")\n",
    "ws2.append([\"metric\", \"value\"])\n",
    "ws2.append([\"threshold\", m_test[\"threshold\"]])\n",
    "ws2.append([\"precision\", m_test[\"precision\"]])\n",
    "ws2.append([\"recall\", m_test[\"recall\"]])\n",
    "ws2.append([\"f1\", m_test[\"f1\"]])\n",
    "ws2.append([\"acc\", m_test[\"acc\"]])\n",
    "ws2.append([\"tp\", m_test[\"tp\"]])\n",
    "ws2.append([\"fp\", m_test[\"fp\"]])\n",
    "ws2.append([\"tn\", m_test[\"tn\"]])\n",
    "ws2.append([\"fn\", m_test[\"fn\"]])\n",
    "ws2.append([\"pos_pred_rate\", m_test[\"pos_pred_rate\"]])\n",
    "ws2.append([\"avg_prob_pos\", m_test[\"avg_prob_pos\"]])\n",
    "ws2.append([\"avg_prob_neg\", m_test[\"avg_prob_neg\"]])\n",
    "ws2.append([\"pr_auc_ap\", ap_test])\n",
    "ws2.append([\"roc_auc\", auc_test])\n",
    "\n",
    "ws2.append([\"\"])\n",
    "ws2.append([\"classification_report\", \"\"])\n",
    "report_str = simple_classification_report(y_true_b, y_pred_b, target_names=[\"Non-cut\", \"Cut\"])\n",
    "for line in report_str.splitlines():\n",
    "    ws2.append([line, \"\"])\n",
    "\n",
    "wb.save(out_path)\n",
    "print(f\"\\nSaved metrics Excel to: {out_path}\")\n",
    "print(f\"[Report] Folder: {REPORT_DIR}\")\n",
    "\n",
    "# ===== 保存模型到同一个报告文件夹（对齐你之前的保存习惯）=====\n",
    "try:\n",
    "    ts_model = datetime.now(_TZ).strftime(\"%Y%m%d_%H%M%S\") if _TZ else datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_path = os.path.join(REPORT_DIR, f\"linear_model_{ts_model}.pt\")\n",
    "    ckpt_path  = os.path.join(REPORT_DIR, f\"linear_ckpt_{ts_model}.pth\")\n",
    "\n",
    "    torch.save(baseline_model.state_dict(), model_path)\n",
    "\n",
    "    ckpt = {\n",
    "        \"model_state_dict\": baseline_model.state_dict(),\n",
    "        \"model_name\": \"BoundaryLinearBaseline\",\n",
    "        \"device_saved\": str(device),\n",
    "        \"threshold_default\": float(threshold_default),\n",
    "        \"batch_size\": int(BATCH_SIZE),\n",
    "        \"report_dir\": REPORT_DIR,\n",
    "        \"report_folder_name\": REPORT_FOLDER_NAME,\n",
    "        \"epochs\": int(epochs),\n",
    "        \"pos_weight\": float(pos_weight),\n",
    "        \"raw_pos_weight\": float(raw_pos_weight) if raw_pos_weight is not None else 0.0,\n",
    "        \"saved_time\": datetime.now(_TZ).strftime(\"%Y-%m-%d %H:%M:%S %Z\") if _TZ else datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    }\n",
    "    torch.save(ckpt, ckpt_path)\n",
    "\n",
    "    print(f\"[Model] Saved state_dict to: {model_path}\")\n",
    "    print(f\"[Model] Saved checkpoint to: {ckpt_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"[Model] Save failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da16983-c669-4ccd-824a-596f9607fc69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============= Cell4: cut_test 测试（详细报告 + 写Excel，严格对齐“代码4.docx 里的原Cell4格式”） =============\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datetime import datetime\n",
    "import platform\n",
    "import time\n",
    "\n",
    "try:\n",
    "    from zoneinfo import ZoneInfo  # py>=3.9\n",
    "    _TZ = ZoneInfo(\"Asia/BeiJing\")\n",
    "except Exception:\n",
    "    _TZ = None\n",
    "\n",
    "# ===== 路径改成你的测试集 =====\n",
    "test_video_dir  = f\"{PROJECT_ROOT}/{DATA_VERSION}/dataset/cut_test\"\n",
    "test_excel_file = f\"{PROJECT_ROOT}/{DATA_VERSION}/dataset/cut_test.xlsx\"\n",
    "\n",
    "# ===== 一些参数 =====\n",
    "threshold_default = 0.95\n",
    "batch_size = 1024\n",
    "topk_suspects = 10  # 每个视频输出 topK “最可疑的 FP / 最可疑的 FN”\n",
    "\n",
    "# ===== 输入尺寸：复用你Cell1/Cell3的 frame_size；没有就兜底 224 =====\n",
    "if \"frame_size\" in globals() and isinstance(frame_size, (tuple, list)) and len(frame_size) == 2:\n",
    "    _FRAME_SIZE = (int(frame_size[0]), int(frame_size[1]))\n",
    "else:\n",
    "    _FRAME_SIZE = (224, 224)\n",
    "\n",
    "# ===== 设备与模型 =====\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# 兼容：baseline_model（单层）或 boundary_model（CNN）\n",
    "_model = None\n",
    "_model_name = \"unavailable\"\n",
    "if \"baseline_model\" in globals():\n",
    "    _model = baseline_model\n",
    "    _model_name = \"baseline_model\"\n",
    "elif \"boundary_model\" in globals():\n",
    "    _model = boundary_model\n",
    "    _model_name = \"boundary_model\"\n",
    "\n",
    "if _model is None:\n",
    "    raise RuntimeError(\"No model found: baseline_model / boundary_model 都不存在。请先运行你的训练Cell。\")\n",
    "\n",
    "_model = _model.to(device)\n",
    "_model.eval()\n",
    "\n",
    "# =====（如果存在你的 simple_classification_report，就复用；否则按原docx补一个最简版）=====\n",
    "if \"simple_classification_report\" not in globals():\n",
    "    def simple_classification_report(y_true, y_pred, target_names):\n",
    "        y_true = np.array(y_true)\n",
    "        y_pred = np.array(y_pred)\n",
    "        n_classes = len(target_names)\n",
    "        lines = []\n",
    "        acc = (y_true == y_pred).sum() / len(y_true) if len(y_true) > 0 else 0.0\n",
    "        lines.append(\"precision    recall  f1-score   support\")\n",
    "        for i in range(n_classes):\n",
    "            name = target_names[i]\n",
    "            true_i = (y_true == i)\n",
    "            pred_i = (y_pred == i)\n",
    "            tp = np.logical_and(true_i, pred_i).sum()\n",
    "            fp = np.logical_and(~true_i, pred_i).sum()\n",
    "            fn = np.logical_and(true_i, ~pred_i).sum()\n",
    "            support = true_i.sum()\n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "            recall    = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "            f1        = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "            lines.append(f\"{name:10s}  {precision:0.4f}   {recall:0.4f}   {f1:0.4f}   {support:5d}\")\n",
    "        lines.append(f\"\\naccuracy                        {acc:0.4f}   {len(y_true):5d}\")\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "# ===== 指标函数（与原docx一致风格）=====\n",
    "def _safe_div(a, b):\n",
    "    return float(a) / float(b) if b else 0.0\n",
    "\n",
    "def binary_metrics_from_probs(y_true, prob_pos, threshold=0.95):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    prob_pos = np.asarray(prob_pos, dtype=np.float64)\n",
    "    y_pred = (prob_pos >= threshold).astype(np.int64)\n",
    "\n",
    "    tp = int(np.sum((y_true == 1) & (y_pred == 1)))\n",
    "    fp = int(np.sum((y_true == 0) & (y_pred == 1)))\n",
    "    tn = int(np.sum((y_true == 0) & (y_pred == 0)))\n",
    "    fn = int(np.sum((y_true == 1) & (y_pred == 0)))\n",
    "\n",
    "    precision = _safe_div(tp, tp + fp)\n",
    "    recall    = _safe_div(tp, tp + fn)\n",
    "    f1        = _safe_div(2 * precision * recall, precision + recall)\n",
    "    acc       = _safe_div(tp + tn, tp + tn + fp + fn)\n",
    "    pos_pred_rate = _safe_div(tp + fp, len(y_true))\n",
    "\n",
    "    avg_prob_pos = float(np.mean(prob_pos[y_true == 1])) if np.any(y_true == 1) else 0.0\n",
    "    avg_prob_neg = float(np.mean(prob_pos[y_true == 0])) if np.any(y_true == 0) else 0.0\n",
    "\n",
    "    return {\n",
    "        \"threshold\": float(threshold),\n",
    "        \"tp\": tp, \"fp\": fp, \"tn\": tn, \"fn\": fn,\n",
    "        \"precision\": precision, \"recall\": recall, \"f1\": f1, \"acc\": acc,\n",
    "        \"pos_pred_rate\": pos_pred_rate,\n",
    "        \"avg_prob_pos\": avg_prob_pos,\n",
    "        \"avg_prob_neg\": avg_prob_neg,\n",
    "        \"y_pred\": y_pred,\n",
    "    }\n",
    "\n",
    "def average_precision_score(y_true, prob_pos):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    prob_pos = np.asarray(prob_pos, dtype=np.float64)\n",
    "    pos_count = int(np.sum(y_true == 1))\n",
    "    if pos_count == 0:\n",
    "        return 0.0\n",
    "    order = np.argsort(-prob_pos)\n",
    "    y_sorted = y_true[order]\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    precisions_at_hits = []\n",
    "    for i in range(len(y_sorted)):\n",
    "        if y_sorted[i] == 1:\n",
    "            tp += 1\n",
    "            precisions_at_hits.append(tp / (tp + fp))\n",
    "        else:\n",
    "            fp += 1\n",
    "    return float(np.sum(precisions_at_hits) / pos_count)\n",
    "\n",
    "def roc_auc_score_rank(y_true, prob_pos):\n",
    "    y_true = np.asarray(y_true, dtype=np.int64)\n",
    "    prob_pos = np.asarray(prob_pos, dtype=np.float64)\n",
    "    n_pos = int(np.sum(y_true == 1))\n",
    "    n_neg = int(np.sum(y_true == 0))\n",
    "    if n_pos == 0 or n_neg == 0:\n",
    "        return 0.0\n",
    "\n",
    "    order = np.argsort(prob_pos)\n",
    "    ranks = np.empty_like(order, dtype=np.float64)\n",
    "    ranks[order] = np.arange(1, len(prob_pos) + 1, dtype=np.float64)\n",
    "\n",
    "    sorted_scores = prob_pos[order]\n",
    "    i = 0\n",
    "    while i < len(sorted_scores):\n",
    "        j = i\n",
    "        while j + 1 < len(sorted_scores) and sorted_scores[j + 1] == sorted_scores[i]:\n",
    "            j += 1\n",
    "        if j > i:\n",
    "            avg_rank = float(np.mean(ranks[order[i:j+1]]))\n",
    "            ranks[order[i:j+1]] = avg_rank\n",
    "        i = j + 1\n",
    "\n",
    "    sum_ranks_pos = float(np.sum(ranks[y_true == 1]))\n",
    "    auc = (sum_ranks_pos - n_pos * (n_pos + 1) / 2.0) / (n_pos * n_neg)\n",
    "    return float(auc)\n",
    "\n",
    "# ===== 读取 Excel（第一行 FPS，后面每行=一个视频的多列cut标注）=====\n",
    "wb = openpyxl.load_workbook(test_excel_file, data_only=True)\n",
    "ws = wb.active\n",
    "rows = list(ws.iter_rows(values_only=True))\n",
    "if len(rows) == 0:\n",
    "    raise RuntimeError(\"cut_test.xlsx 里没有任何行！\")\n",
    "\n",
    "fps_row = rows[0]\n",
    "\n",
    "def get_fps_from_row(r, default=24.0):\n",
    "    for cell in r:\n",
    "        if cell is None:\n",
    "            continue\n",
    "        if isinstance(cell, (int, float)):\n",
    "            return float(cell)\n",
    "        if isinstance(cell, str):\n",
    "            s = cell.strip()\n",
    "            if s == \"\" or s.lower() == \"none\":\n",
    "                continue\n",
    "            try:\n",
    "                return float(s)\n",
    "            except Exception:\n",
    "                continue\n",
    "    return float(default)\n",
    "\n",
    "fps = get_fps_from_row(fps_row, default=24.0)\n",
    "print(f\"[cut_test] Using FPS from Excel first row: {fps}\")\n",
    "\n",
    "data_rows = rows[1:]\n",
    "\n",
    "video_files = sorted(glob.glob(f\"{test_video_dir}/V*.mp4\"))\n",
    "assert len(video_files) == len(data_rows), f\"Mismatch: videos({len(video_files)}) vs excel rows({len(data_rows)})\"\n",
    "\n",
    "def timecode_to_frame(tc, fps):\n",
    "    if tc is None:\n",
    "        return None\n",
    "    if isinstance(tc, (int, float)):\n",
    "        return int(tc)\n",
    "    if isinstance(tc, str):\n",
    "        s = tc.strip()\n",
    "        if s == \"\" or s.lower() == \"none\":\n",
    "            return None\n",
    "        if s.isdigit():\n",
    "            return int(s)\n",
    "        if \":\" in s:\n",
    "            parts = s.split(\":\")\n",
    "            try:\n",
    "                if len(parts) == 2:\n",
    "                    sec = int(parts[0])\n",
    "                    frm = int(parts[1])\n",
    "                    return int(sec * fps + frm)\n",
    "                elif len(parts) == 3:\n",
    "                    h = int(parts[0]); m = int(parts[1]); sec = int(parts[2])\n",
    "                    total_sec = h * 3600 + m * 60 + sec\n",
    "                    return int(total_sec * fps)\n",
    "                else:\n",
    "                    return None\n",
    "            except Exception:\n",
    "                return None\n",
    "    return None\n",
    "\n",
    "# ===== 读取视频帧 + 生成 boundary_pairs_test（完全对齐原Cell4逻辑：Excel标B-start -> pair=i=B-1）=====\n",
    "video_frames = []\n",
    "boundary_pairs_test = []\n",
    "video_meta = []  # 每个视频的 meta：total_frames, gt_cut_indices, name, path\n",
    "\n",
    "for vid_idx, (video_path, row) in enumerate(zip(video_files, data_rows)):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    success, frame = cap.read()\n",
    "    while success:\n",
    "        frame_resized = cv2.resize(frame, _FRAME_SIZE)\n",
    "        frames.append(frame_resized)\n",
    "        success, frame = cap.read()\n",
    "    cap.release()\n",
    "\n",
    "    video_frames.append(frames)\n",
    "    total_frames = len(frames)\n",
    "\n",
    "    if total_frames <= 1:\n",
    "        print(f\"[cut_test] Warning: {video_path} has {total_frames} frame(s), skip.\")\n",
    "        video_meta.append({\n",
    "            \"vid_idx\": vid_idx,\n",
    "            \"name\": os.path.basename(video_path),\n",
    "            \"path\": video_path,\n",
    "            \"total_frames\": total_frames,\n",
    "            \"gt_cut_indices\": [],\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    raw_values = list(row) if row is not None else []\n",
    "    cut_indices = []\n",
    "\n",
    "    for v in raw_values:\n",
    "        frame_idx = timecode_to_frame(v, fps)\n",
    "        if frame_idx is None:\n",
    "            continue\n",
    "\n",
    "        # Excel 标切后起始帧(B-start)，SBD 标在 (B-1,B) -> i=B-1\n",
    "        if frame_idx > 0:\n",
    "            frame_idx = frame_idx - 1\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        frame_idx = int(frame_idx)\n",
    "        frame_idx = max(0, min(frame_idx, total_frames - 2))\n",
    "        cut_indices.append(frame_idx)\n",
    "\n",
    "    cut_indices = sorted(set(cut_indices))\n",
    "    print(f\"[cut_test] Video {vid_idx} ({os.path.basename(video_path)}): total_frames={total_frames}, cuts-1@frames={cut_indices}\")\n",
    "\n",
    "    video_meta.append({\n",
    "        \"vid_idx\": vid_idx,\n",
    "        \"name\": os.path.basename(video_path),\n",
    "        \"path\": video_path,\n",
    "        \"total_frames\": total_frames,\n",
    "        \"gt_cut_indices\": cut_indices,\n",
    "    })\n",
    "\n",
    "    cut_set = set(cut_indices)\n",
    "    for i in range(total_frames - 1):\n",
    "        label = 1 if i in cut_set else 0\n",
    "        boundary_pairs_test.append((vid_idx, i, label))\n",
    "\n",
    "num_pairs = len(boundary_pairs_test)\n",
    "num_cuts = sum(1 for _, _, lbl in boundary_pairs_test if lbl == 1)\n",
    "num_noncuts = num_pairs - num_cuts\n",
    "print(f\"\\n[cut_test] Generated {num_pairs} pairs: {num_cuts} cuts, {num_noncuts} non-cuts\")\n",
    "\n",
    "# ===== Dataset：9通道(frameA, frameB, diff)（对齐原Cell4）=====\n",
    "class ShotBoundaryDataset(Dataset):\n",
    "    def __init__(self, pairs, video_frames):\n",
    "        self.pairs = pairs\n",
    "        self.video_frames = video_frames\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        vid_idx, frame_idx, label = self.pairs[idx]\n",
    "        frames = self.video_frames[vid_idx]\n",
    "        T = len(frames)\n",
    "\n",
    "        # 4-frame context (clamp to avoid out-of-range)\n",
    "        i_prev = max(0, frame_idx - 1)\n",
    "        i_A    = max(0, min(frame_idx,     T - 1))\n",
    "        i_B    = max(0, min(frame_idx + 1, T - 1))\n",
    "        i_next = max(0, min(frame_idx + 2, T - 1))\n",
    "\n",
    "        framePrev = frames[i_prev]\n",
    "        frameA    = frames[i_A]\n",
    "        frameB    = frames[i_B]\n",
    "        frameNext = frames[i_next]\n",
    "\n",
    "        diff1 = cv2.absdiff(framePrev, frameA)\n",
    "        diff2 = cv2.absdiff(frameA, frameB)\n",
    "        diff3 = cv2.absdiff(frameB, frameNext)\n",
    "\n",
    "        # 21ch = prev(3)+A(3)+B(3)+next(3)+diff(prev,A)(3)+diff(A,B)(3)+diff(B,next)(3)\n",
    "        img_21ch = np.concatenate([framePrev, frameA, frameB, frameNext, diff1, diff2, diff3], axis=2).astype(\"float32\") / 255.0\n",
    "        img_chw = np.transpose(img_21ch, (2, 0, 1))\n",
    "\n",
    "        img_tensor = torch.tensor(img_chw, dtype=torch.float32)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "        return img_tensor, label_tensor\n",
    "\n",
    "test_dataset = ShotBoundaryDataset(boundary_pairs_test, video_frames)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# ===== 推理并收集：整体 + 每个pair的详细信息（对齐原Cell4）=====\n",
    "y_true, prob_pos, y_pred = [], [], []\n",
    "pair_details = []  # (vid_idx, frame_idx, gt, prob, pred)\n",
    "\n",
    "t0 = time.time()\n",
    "pair_ptr = 0\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_loader:\n",
    "        bsz = labels.size(0)\n",
    "        imgs = imgs.to(device)\n",
    "        out = _model(imgs)\n",
    "\n",
    "        # 兼容 CNN logits[B,2] / baseline 单logit[B] 或 [B,1]\n",
    "        if out.dim() == 2 and out.size(1) == 2:\n",
    "            probs = torch.softmax(out, dim=1)[:, 1]\n",
    "        else:\n",
    "            probs = torch.sigmoid(out.view(-1))\n",
    "\n",
    "        pred = (probs >= threshold_default).long()\n",
    "\n",
    "        probs_np = probs.detach().cpu().numpy()\n",
    "        pred_np  = pred.detach().cpu().numpy()\n",
    "        labels_np = labels.detach().cpu().numpy()\n",
    "\n",
    "        y_true.extend(labels_np.tolist())\n",
    "        prob_pos.extend(probs_np.tolist())\n",
    "        y_pred.extend(pred_np.tolist())\n",
    "\n",
    "        for j in range(bsz):\n",
    "            vid_idx, frame_idx, gt = boundary_pairs_test[pair_ptr + j]\n",
    "            pair_details.append((vid_idx, frame_idx, int(gt), float(probs_np[j]), int(pred_np[j])))\n",
    "        pair_ptr += bsz\n",
    "\n",
    "t_infer = time.time() - t0\n",
    "\n",
    "# ===== 汇总整体指标 =====\n",
    "m = binary_metrics_from_probs(y_true, prob_pos, threshold=threshold_default)\n",
    "ap = average_precision_score(y_true, prob_pos)\n",
    "auc = roc_auc_score_rank(y_true, prob_pos)\n",
    "\n",
    "print(\"\\n[cut_test] FINAL TEST (threshold=0.5) => \"\n",
    "      f\"F1 {m['f1']:.4f} (P {m['precision']:.4f}, R {m['recall']:.4f}) | \"\n",
    "      f\"AP {ap:.4f} | AUC {auc:.4f} | \"\n",
    "      f\"TP {m['tp']} FP {m['fp']} TN {m['tn']} FN {m['fn']}\")\n",
    "\n",
    "print(\"\\nShot Boundary Detection - Classification Report (cut_test):\")\n",
    "report_str = simple_classification_report(y_true, y_pred, target_names=[\"Non-cut\", \"Cut\"])\n",
    "print(report_str)\n",
    "\n",
    "# ===== 每个视频的详细统计（对齐原Cell4：per_video + suspects_topk）=====\n",
    "per_video_pairs = {vm[\"vid_idx\"]: [] for vm in video_meta}\n",
    "for (vid_idx, frame_idx, gt, p, pred) in pair_details:\n",
    "    per_video_pairs[vid_idx].append((frame_idx, gt, p, pred))\n",
    "\n",
    "per_video_rows = []\n",
    "per_video_suspects_rows = []  # [vid, vid_idx, type, rank, frame_idx, prob_cut]\n",
    "\n",
    "for vm in video_meta:\n",
    "    vid_idx = vm[\"vid_idx\"]\n",
    "    name = vm[\"name\"]\n",
    "    total_frames = vm[\"total_frames\"]\n",
    "    gt_cuts = set(vm[\"gt_cut_indices\"])\n",
    "\n",
    "    pairs = per_video_pairs.get(vid_idx, [])\n",
    "    if not pairs:\n",
    "        per_video_rows.append({\n",
    "            \"vid\": name, \"vid_idx\": vid_idx, \"total_frames\": total_frames,\n",
    "            \"gt_cut_count\": len(gt_cuts), \"pred_cut_count\": 0,\n",
    "            \"tp\": 0, \"fp\": 0, \"fn\": len(gt_cuts),\n",
    "            \"gt_cuts\": \",\".join(map(str, sorted(gt_cuts))) if gt_cuts else \"none\",\n",
    "            \"pred_cuts\": \"none\",\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    pred_cuts = sorted({fr for (fr, gt, p, pred) in pairs if pred == 1})\n",
    "    pred_cut_set = set(pred_cuts)\n",
    "\n",
    "    tp = len(gt_cuts & pred_cut_set)\n",
    "    fp = len(pred_cut_set - gt_cuts)\n",
    "    fn = len(gt_cuts - pred_cut_set)\n",
    "\n",
    "    per_video_rows.append({\n",
    "        \"vid\": name, \"vid_idx\": vid_idx, \"total_frames\": total_frames,\n",
    "        \"gt_cut_count\": len(gt_cuts), \"pred_cut_count\": len(pred_cut_set),\n",
    "        \"tp\": tp, \"fp\": fp, \"fn\": fn,\n",
    "        \"gt_cuts\": \",\".join(map(str, sorted(gt_cuts))) if gt_cuts else \"none\",\n",
    "        \"pred_cuts\": \",\".join(map(str, pred_cuts)) if pred_cuts else \"none\",\n",
    "    })\n",
    "\n",
    "    fps_list = [(fr, p) for (fr, gt, p, pred) in pairs if gt == 0 and pred == 1]\n",
    "    fns_list = [(fr, p) for (fr, gt, p, pred) in pairs if gt == 1 and pred == 0]\n",
    "    fps_list = sorted(fps_list, key=lambda x: -x[1])[:topk_suspects]\n",
    "    fns_list = sorted(fns_list, key=lambda x: x[1])[:topk_suspects]\n",
    "\n",
    "    for rank, (fr, pp) in enumerate(fps_list, start=1):\n",
    "        per_video_suspects_rows.append([name, vid_idx, \"FP\", rank, fr, float(pp)])\n",
    "    for rank, (fr, pp) in enumerate(fns_list, start=1):\n",
    "        per_video_suspects_rows.append([name, vid_idx, \"FN\", rank, fr, float(pp)])\n",
    "\n",
    "# ===== 写入 Excel：sheet 名与结构严格对齐原Cell4 =====\n",
    "base_dir = \"movie/reports\"\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "# 复用训练Cell3产生的报告目录；没有就按同规则新建（原Cell4就是这么做的）\n",
    "if \"REPORT_DIR\" not in globals() or \"REPORT_FOLDER_NAME\" not in globals():\n",
    "    ts_folder = datetime.now(_TZ).strftime(\"%m%d%H%M\") if _TZ else datetime.now().strftime(\"%m%d%H%M\")\n",
    "    _e = globals().get(\"epochs\", \"NA\")\n",
    "    _w = globals().get(\"pos_weight\", \"NA\")\n",
    "    REPORT_FOLDER_NAME = f\"{ts_folder}_e{_e}_w{_w}\"\n",
    "    REPORT_DIR = os.path.join(base_dir, REPORT_FOLDER_NAME)\n",
    "    os.makedirs(REPORT_DIR, exist_ok=True)\n",
    "\n",
    "ts = datetime.now(_TZ).strftime(\"%Y%m%d_%H%M%S\") if _TZ else datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "out_path = os.path.join(REPORT_DIR, f\"cut_test_report_{ts}.xlsx\")\n",
    "\n",
    "wb_out = openpyxl.Workbook()\n",
    "\n",
    "# Sheet: run_info（原Cell4格式）\n",
    "ws0 = wb_out.active\n",
    "ws0.title = \"run_info\"\n",
    "ws0.append([\"key\", \"value\"])\n",
    "ws0.append([\"time\", datetime.now(_TZ).strftime(\"%Y-%m-%d %H:%M:%S %Z\") if _TZ else datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")])\n",
    "ws0.append([\"python_version\", platform.python_version()])\n",
    "ws0.append([\"platform\", platform.platform()])\n",
    "ws0.append([\"processor\", platform.processor()])\n",
    "ws0.append([\"torch_version\", torch.__version__])\n",
    "ws0.append([\"cuda_available\", str(torch.cuda.is_available())])\n",
    "ws0.append([\"device_used\", str(device)])\n",
    "ws0.append([\"test_video_dir\", test_video_dir])\n",
    "ws0.append([\"test_excel_file\", test_excel_file])\n",
    "ws0.append([\"fps_from_excel\", str(fps)])\n",
    "ws0.append([\"threshold_default\", str(threshold_default)])\n",
    "ws0.append([\"batch_size\", str(batch_size)])\n",
    "ws0.append([\"topk_suspects\", str(topk_suspects)])\n",
    "ws0.append([\"model_used\", _model_name])\n",
    "ws0.append([\"infer_seconds\", f\"{t_infer:.3f}\"])\n",
    "ws0.append([\"report_folder_name\", REPORT_FOLDER_NAME])\n",
    "ws0.append([\"report_dir\", REPORT_DIR])\n",
    "\n",
    "# Sheet: dataset_summary（原Cell4格式）\n",
    "ws_sum = wb_out.create_sheet(\"dataset_summary\")\n",
    "ws_sum.append([\"item\", \"value\"])\n",
    "ws_sum.append([\"num_videos\", len(video_files)])\n",
    "ws_sum.append([\"num_pairs\", num_pairs])\n",
    "ws_sum.append([\"num_cuts\", num_cuts])\n",
    "ws_sum.append([\"num_non_cuts\", num_noncuts])\n",
    "ws_sum.append([\"pos_ratio\", (num_cuts / num_pairs) if num_pairs else 0.0])\n",
    "\n",
    "ws_sum.append([\"\"])\n",
    "ws_sum.append([\"per_video_frame_stats\", \"\"])\n",
    "frames_list = [vm[\"total_frames\"] for vm in video_meta if vm[\"total_frames\"] is not None]\n",
    "if frames_list:\n",
    "    ws_sum.append([\"min_frames\", int(np.min(frames_list))])\n",
    "    ws_sum.append([\"max_frames\", int(np.max(frames_list))])\n",
    "    ws_sum.append([\"mean_frames\", float(np.mean(frames_list))])\n",
    "    ws_sum.append([\"median_frames\", float(np.median(frames_list))])\n",
    "\n",
    "# Sheet: per_video（原Cell4格式）\n",
    "ws_v = wb_out.create_sheet(\"per_video\")\n",
    "ws_v.append([\n",
    "    \"vid\", \"vid_idx\", \"total_frames\",\n",
    "    \"gt_cut_count\", \"pred_cut_count\",\n",
    "    \"tp\", \"fp\", \"fn\",\n",
    "    \"gt_cuts\", \"pred_cuts\"\n",
    "])\n",
    "for r in per_video_rows:\n",
    "    ws_v.append([\n",
    "        r[\"vid\"], r[\"vid_idx\"], r[\"total_frames\"],\n",
    "        r[\"gt_cut_count\"], r[\"pred_cut_count\"],\n",
    "        r[\"tp\"], r[\"fp\"], r[\"fn\"],\n",
    "        r[\"gt_cuts\"], r[\"pred_cuts\"]\n",
    "    ])\n",
    "\n",
    "# Sheet: suspects_topk（原Cell4格式）\n",
    "ws_sus = wb_out.create_sheet(\"suspects_topk\")\n",
    "ws_sus.append([\"vid\", \"vid_idx\", \"type\", \"rank\", \"frame_idx\", \"prob_cut\"])\n",
    "for row in per_video_suspects_rows:\n",
    "    ws_sus.append(row)\n",
    "\n",
    "# Sheet: final_test（原Cell4格式）\n",
    "ws2 = wb_out.create_sheet(\"final_test\")\n",
    "ws2.append([\"metric\", \"value\"])\n",
    "ws2.append([\"threshold\", m[\"threshold\"]])\n",
    "ws2.append([\"precision\", m[\"precision\"]])\n",
    "ws2.append([\"recall\", m[\"recall\"]])\n",
    "ws2.append([\"f1\", m[\"f1\"]])\n",
    "ws2.append([\"acc\", m[\"acc\"]])\n",
    "ws2.append([\"tp\", m[\"tp\"]])\n",
    "ws2.append([\"fp\", m[\"fp\"]])\n",
    "ws2.append([\"tn\", m[\"tn\"]])\n",
    "ws2.append([\"fn\", m[\"fn\"]])\n",
    "ws2.append([\"pos_pred_rate\", m[\"pos_pred_rate\"]])\n",
    "ws2.append([\"avg_prob_pos\", m[\"avg_prob_pos\"]])\n",
    "ws2.append([\"avg_prob_neg\", m[\"avg_prob_neg\"]])\n",
    "ws2.append([\"pr_auc_ap\", ap])\n",
    "ws2.append([\"roc_auc\", auc])\n",
    "\n",
    "# Sheet: classification_report（原Cell4格式：逐行写文本）\n",
    "ws_rep = wb_out.create_sheet(\"classification_report\")\n",
    "ws_rep.append([\"text\"])\n",
    "for line in report_str.splitlines():\n",
    "    ws_rep.append([line])\n",
    "\n",
    "wb_out.save(out_path)\n",
    "print(f\"\\nSaved cut_test report Excel to: {out_path}\")\n",
    "print(f\"[Report] Folder: {REPORT_DIR}\")\n",
    "\n",
    "# ===== 保存模型到同一个报告文件夹（对齐你原Cell4风格：state_dict + ckpt）=====\n",
    "try:\n",
    "    os.makedirs(REPORT_DIR, exist_ok=True)\n",
    "    ts_model = datetime.now(_TZ).strftime(\"%Y%m%d_%H%M%S\") if _TZ else datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # 文件名保持原风格，但根据模型类型区分一下（不影响你的系统读取Excel）\n",
    "    if _model_name == \"baseline_model\":\n",
    "        model_path = os.path.join(REPORT_DIR, f\"baseline_model_{ts_model}.pt\")\n",
    "        ckpt_path  = os.path.join(REPORT_DIR, f\"baseline_ckpt_{ts_model}.pth\")\n",
    "    else:\n",
    "        model_path = os.path.join(REPORT_DIR, f\"boundary_model_{ts_model}.pt\")\n",
    "        ckpt_path  = os.path.join(REPORT_DIR, f\"boundary_ckpt_{ts_model}.pth\")\n",
    "\n",
    "    torch.save(_model.state_dict(), model_path)\n",
    "\n",
    "    ckpt = {\n",
    "        \"model_state_dict\": _model.state_dict(),\n",
    "        \"model_used\": _model_name,\n",
    "        \"device_saved\": str(device),\n",
    "        \"fps_from_excel\": float(fps),\n",
    "        \"threshold_default\": float(threshold_default),\n",
    "        \"batch_size\": int(batch_size),\n",
    "        \"frame_size\": tuple(_FRAME_SIZE),\n",
    "        \"report_dir\": REPORT_DIR,\n",
    "        \"report_folder_name\": REPORT_FOLDER_NAME,\n",
    "        \"epochs\": globals().get(\"epochs\", None),\n",
    "        \"pos_weight\": globals().get(\"pos_weight\", None),\n",
    "        \"build_id\": globals().get(\"BUILD_ID\", None),\n",
    "        \"saved_time\": datetime.now(_TZ).strftime(\"%Y-%m-%d %H:%M:%S %Z\") if _TZ else datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    }\n",
    "    torch.save(ckpt, ckpt_path)\n",
    "\n",
    "    print(f\"[Model] Saved state_dict to: {model_path}\")\n",
    "    print(f\"[Model] Saved checkpoint to: {ckpt_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[Model] Save failed: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
